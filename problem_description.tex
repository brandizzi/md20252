\subsection{Containers}
\IEEEPARstart{C}{ontainers} are software packages that bundle all components required to execute an application. They operate in isolation from one another and from the host operating system (OS) at runtime, ensuring consistent behavior across heterogeneous environments, including different operating systems and hardware configurations~\cite{moreau_containers_2023}. Each container is distributed as a \textit{container image} and executed by a \textit{container runtime}~\cite{merkel_docker_2014}. Conceptually, a container can be viewed as a lightweight virtual machine (VM) that shares the host kernel in an isolated manner—lean, cost-effective, and fast to create or destroy. This efficiency has made containers popular for providing process isolation while maintaining low resource overhead~\cite{watada_emerging_2019}. Containers are particularly well suited to cloud computing environments~\cite{moreau_containers_2023}.

Managing a large number of containers, however, can be complex. To address this challenge, \textit{container orchestrators} such as Kubernetes are widely employed~\cite{watada_emerging_2019}

\subsection{Orchestrators}

A \textbf{container orchestrator} is a system that determines when and how containers are deployed, managed, and monitored in dynamic environments. It coordinates the execution of containerized applications across multiple nodes, unlike tools such as Docker Compose, which only manage containers on a single host~\cite{casalicchio_container_2019}.

Modern orchestrators can dynamically create and destroy containers and, when integrated with VM providers, can also provision or decommission VM nodes. Their principal functionalities typically include~\cite{casalicchio_container_2019}:

\begin{itemize}
	\item \textbf{Resource management:} Reserving and limiting CPU, memory, and storage resources for individual containers.
	\item \textbf{Health checks:} Continuously verifying that containers operate correctly and automatically restarting failed instances.
	\item \textbf{Scheduling:} Determining the number and placement of container instances across available nodes.
	\item \textbf{Load balancing:} Distributing incoming requests among instances to optimize performance and availability.
	\item \textbf{Auto-scaling:} Adjusting the number of running containers based on real-time performance metrics such as CPU or memory usage.
\end{itemize}

Prominent orchestration platforms include Kubernetes, Docker Swarm, Apache Mesos, Red Hat OpenShift, and HashiCorp Nomad~\cite{malviya_comparative_2022}. Kubernetes is one of the most popular options, and it is the orchestrator of interest for this studymemory-usage-time-container.png.

\subsection{Auto-scaling}

An \textbf{auto-scaling system} (or \textbf{auto-scaler}) is a mechanism that automatically adjusts an application's computational resources—such as CPU, memory, or storage—based on current demand, requiring minimal or no human intervention~\cite{lorido-botran_review_2014}.  

Auto-scalers are commonly divided into two categories: \textbf{horizontal scaling}, which adds or removes instances of a service, and \textbf{vertical scaling}, which adjusts the resources allocated to existing instances.

\textbf{Vertical auto-scaling} seeks to provide optimal resource allocation by increasing or decreasing the capacity of running instances. This adjustment can be achieved by modifying the resources assigned to a VM or container—for example, altering CPU or memory quotas~\cite[p.~560]{lorido-botran_review_2014}—or by recreating instances with updated resource limits. In Kubernetes, the \textit{Vertical Pod Autoscaler} (VPA) can either recreate pods with updated resource allocations or modify pod specifications dynamically while they are running~\cite{lakshmikanthan_optimizing_2021, kubernetes_authors_resize_2025}.

\textbf{Horizontal auto-scaling}, in contrast, optimizes application resource usage by creating or removing instances of the application. In cloud environments, this typically involves provisioning or terminating VMs or containers. In this approach, the resource unit is the server replica (running on a VM or container), and replicas are added or removed according to demand~\cite[p.~560]{lorido-botran_review_2014}.  

In Kubernetes, the \textit{HorizontalPodAutoscaler} (HPA)~\cite{kubernetes_authors_horizontal_2024} implements this approach by adjusting the number of pods based on predefined targets for average CPU or memory utilization across the application's containers~\cite{lakshmikanthan_optimizing_2021}.

\subsection{Liferay DXP}

\textit{Liferay Portal} is an open-source  Java-based platform for developing web portals~\cite{ferrer_juan_alisis_2025}. With over two decades of development, it provides a rich set of features—including content management, object storage, and permission management—and offers extensive customization capabilities that enable the creation of bespoke applications~\cite{criollo_jimenez_desarrollo_2025}. On top of Liferay Portal, Liferay Inc., the company behind the project, provides \textit{Liferay Digital Experience Platform (DXP)}, a commercially supported \textit{digital experience platform} that integrates and executes internal and external business processes~\cite{koppula_liferay_2025}.

Liferay DXP is also offered as a \textit{Platform-as-a-Service} (PaaS) solution, known as \textit{Liferay PaaS}, which operates in Kubernetes clusters hosted on Google Cloud Platform (GCP), specifically on Google Kubernetes Engine (GKE). Because these clusters are shared among customers, most users lack direct access to cluster-level configuration but retain the ability to leverage key Kubernetes features, including HPA for automatic scaling of their Liferay instances.

The use of HPA with Liferay DXP introduces particular challenges. The application is a large, Java-based system with thousands of features and deep extensibility—characteristics that complicate auto-scaling behavior~\cite{lakshmikanthan_optimizing_2021}. In particular, Java Virtual Machines (JVMs) are known to be resource-intensive and slow to release memory~\cite{bruno_study_2019, patricio_garbage_nodate}. Since HPA relies on resource metrics such as memory usage to make scaling decisions, delayed memory release can cause the system to postpone scaling down, leading to the retention of unnecessary instances and increased operational costs~\cite{sullins_jmx_2002}.

Figure~\ref{fig:memory-usage-long-time} illustrates a real-world incident in which a temporary traffic spike caused sustained high memory usage for 24 hours. As a result, HPA maintained an additional pod long after demand had subsided, thereby incurring unnecessary costs. CPU utilization, another commonly used HPA metric, is generally unsuitable for Java web applications, which are typically input/output (I/O)-bound rather than CPU-bound.

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{memory-usage-24h.jpeg}
	\caption{Memory usage during a real incident. A transient traffic spike led to sustained high memory consumption, preventing HPA from scaling down for 24 hours.}
	\label{fig:memory-usage-long-time}
\end{figure}

In addition, Java-based web applications often exhibit long startup times~\cite{lakshmikanthan_optimizing_2021}. Liferay, with its extensive feature set, is no exception: a moderately customized instance may take from two to ten minutes to become fully operational. Consequently, when HPA reacts to increased load, new pods may be launched too late to effectively mitigate performance degradation.

To address these challenges, two strategies are investigated in this study. First, more responsive metrics are employed through the use of custom metrics exposed via a Prometheus adapter~\cite{noauthor_horizontal_2025}, using the JVM heap size as the primary indicator. Second, time-series forecasting algorithms are explored to enable proactive scaling decisions based on predicted demand.
